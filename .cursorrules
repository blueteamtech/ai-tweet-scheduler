# AI Tweet Scheduler v2.0 - Cursor Rules

## Project Context
**MVP COMPLETE** - Building v2.0 Personality AI Enhancement
- v1.0: Basic AI tweet scheduler with Twitter integration ✅
- v2.0: Personality-driven content generation with bulk scheduling
- Reference: `v2-personality-ai-plan.md` for development roadmap

## Tech Stack (v2.0)
- Frontend: Next.js 14 with TypeScript
- Database: Supabase + pgvector (vector embeddings)
- AI: OpenAI GPT-4o + text-embedding-3-small
- Scheduling: QStash by Upstash
- Styling: Tailwind CSS
- Deployment: Vercel

---

## 🚨 CRITICAL SAFEGUARDS (Prevent Hours of Troubleshooting)

### 📚 API INTEGRATION STRATEGY - CHECK EXISTING FIRST
**MANDATORY:** Before ANY API work, check what's already integrated
```
1. EXISTING APIs (Keys Already Set):
   ✅ OpenAI API (OPENAI_API_KEY) - GPT-4o + text-embedding-3-small
   ✅ Supabase (NEXT_PUBLIC_SUPABASE_URL, SUPABASE_SERVICE_ROLE_KEY, etc.)
   ✅ QStash (QSTASH_TOKEN) - Scheduling service
   ✅ Twitter API (TWITTER_API_KEY, TWITTER_API_SECRET) - OAuth + posting

2. ONLY for NEW APIs:
   - web_search: "[API NAME] documentation 2025"
   - Verify parameter formats, response structures, rate limits
   - Check for breaking changes in last 6 months
   - Get API keys from user

3. For EXISTING APIs:
   - Assume keys are already configured
   - Check current documentation for updates/changes
   - Verify we're using latest best practices
```

### 🗄️ DATABASE MANAGEMENT STRATEGY
**CONTEXT:** `database-schema.md` documents LIVE database state for development context
**IMPORTANT:** User still applies migration files manually to update live Supabase database

**WORKFLOW:**
1. **Local Development:**
   - Update `database-schema.md` to reflect planned changes (documentation only)
   - Create migration SQL files (e.g., `v2-phase1-database-migration.sql`)
   - Create rollback commands in `database-rollback-commands.md`
   - Test rollback SQL locally first

2. **Live Database Updates:**
   - User manually runs migration SQL in Supabase SQL Editor
   - User applies environment variable changes to Vercel
   - `database-schema.md` stays in sync with live database for reference

3. **Database Migration Pattern:**
   ```sql
   -- FORWARD MIGRATION (user runs in Supabase)
   CREATE TABLE user_writing_samples (...);
   
   -- ROLLBACK (test these first!):
   -- DROP TABLE IF EXISTS user_writing_samples CASCADE;
   ```

4. **Always create rollback before forward migration**

### 🚀 VERCEL DEPLOYMENT STATE TRACKING
**PROBLEM:** Environment variables and database can get out of sync
**SOLUTION:**
1. **Before deploying:** Document current state in `deployment-state.md`
2. **Environment Variables:** Keep local `.env.local` and Vercel settings in sync
3. **Database Schema:** Ensure Supabase matches local SQL files
4. **Rollback Plan:** Document how to revert Vercel + Database to previous working state

---

## 🔍 CODE QUALITY & ESLINT PREVENTION
**MANDATORY:** Prevent build failures by following these rules BEFORE committing

### **ESLint Rules to Follow:**
1. **TypeScript Strictness:**
   - ❌ NEVER use `any` type - create proper interfaces instead
   - ✅ Use `const` for variables that are never reassigned (not `let`)
   - ✅ Define proper types for function parameters and returns
   - ✅ Use type guards for runtime type checking

2. **React/JSX Rules:**
   - ❌ NEVER use unescaped quotes in JSX: `"text"` 
   - ✅ Always escape quotes: `&quot;text&quot;` or `&ldquo;text&rdquo;`
   - ✅ Use proper event handler types: `React.MouseEvent`, `React.ChangeEvent`
   - ✅ Include `key` props for mapped elements

3. **Variable Declaration:**
   - ✅ Use `const` for values that never change
   - ✅ Use `let` only when the variable will be reassigned
   - ❌ Avoid `var` completely

### **Pre-Commit Checklist:**
Before ANY git commit, verify:
- [ ] No TypeScript errors (`any` types, missing interfaces)
- [ ] No unescaped entities in JSX (quotes, apostrophes)
- [ ] Proper const/let usage (prefer const when possible)
- [ ] All imports are used and correctly typed
- [ ] Event handlers have proper TypeScript types

### **Common Pitfalls & Fixes:**
```typescript
// ❌ BAD: Using 'any' type
const items = data.map((item: any) => ({ ... }))

// ✅ GOOD: Proper interface
interface DatabaseItem { id: string; name: string; }
const items = data.map((item: DatabaseItem) => ({ ... }))

// ❌ BAD: Using 'let' when never reassigned
let config = { apiKey: '...', timeout: 5000 }

// ✅ GOOD: Using 'const' 
const config = { apiKey: '...', timeout: 5000 }

// ❌ BAD: Unescaped quotes in JSX
<span>Click "Save" to continue</span>

// ✅ GOOD: Escaped quotes
<span>Click &quot;Save&quot; to continue</span>
```

### **Build Validation Process:**
1. **Local Testing:** Run `npm run build` locally before pushing
2. **Type Check:** Ensure all TypeScript types are properly defined
3. **ESLint Clean:** No warnings or errors in ESLint output
4. **Import Check:** All imports are used and paths are correct

---

## 🔧 V2.0 SPECIFIC RULES

### OpenAI Embeddings (v2.0)
- **Model:** `text-embedding-3-small` (check current pricing)
- **Dimensions:** 1536 (verify current default)
- **Rate Limits:** Check current limits before bulk operations
- **Cost Management:** Estimate costs for embedding user writing samples

### Supabase pgvector (v2.0)
- **Extension:** Enable pgvector extension first
- **Vector Storage:** `embedding vector(1536)` (verify dimensions)
- **Similarity Search:** Use cosine similarity for semantic matching
- **Performance:** Add indexes for user_id + similarity queries

### Bulk Processing (v2.0)
- **Rate Limiting:** Implement delays between API calls
- **Error Handling:** Graceful failure for bulk operations
- **Progress Tracking:** Show progress for long-running operations
- **Cancellation:** Allow users to cancel bulk operations

---

## 📋 V2.0 DEVELOPMENT WORKFLOW

### Before Starting ANY Phase:
1. ✅ Check `v2-personality-ai-plan.md` checklist
2. ✅ Check current API documentation (web_search)
3. ✅ Update `database-schema.md` with planned changes
4. ✅ Create rollback commands for database changes
5. ✅ Test rollback commands before implementing forward changes

### Database Changes:
1. **Plan:** Update `database-schema.md` first (documentation only)
2. **Create Migration:** Write SQL migration file (e.g., `v2-phase1-database-migration.sql`)
3. **Rollback:** Create and test rollback SQL in `database-rollback-commands.md`
4. **User Applies:** User runs migration SQL in Supabase SQL Editor manually
5. **Verify:** Run verification SQL to confirm changes
6. **Document:** Add to recent changes with date in `database-schema.md`

### API Integration:
1. **Research:** web_search for current documentation
2. **Debug Route:** Create simple test endpoint first
3. **Parameters:** Verify formats match current docs
4. **Error Handling:** Handle rate limits and failures
5. **Testing:** Test edge cases and error scenarios

---

## 🗂️ FILE ORGANIZATION (v2.0)

```
AI-Personality-Tweets-to-Scheduler/
├── v2-personality-ai-plan.md          # Development roadmap
├── database-schema.md                 # Schema documentation
├── database-rollback-commands.md      # Rollback procedures
├── deployment-state.md                # Vercel/DB state tracking
└── ai-tweet-scheduler/
    ├── src/
    │   ├── components/
    │   │   ├── WritingSampleInput.tsx  # v2.0: Writing analysis
    │   │   ├── BulkTweetInput.tsx      # v2.0: Bulk generation
    │   │   └── TweetQueue.tsx          # v2.0: Queue management
    │   ├── lib/
    │   │   ├── embeddings.ts           # v2.0: OpenAI embeddings
    │   │   ├── similarity.ts           # v2.0: Vector similarity
    │   │   └── bulk-scheduler.ts       # v2.0: Smart scheduling
    │   └── app/api/
    │       ├── analyze-writing/        # v2.0: Writing analysis
    │       ├── bulk-generate-tweets/   # v2.0: Bulk generation
    │       └── bulk-schedule-tweets/   # v2.0: Bulk scheduling
```

---

## ⚠️ COMMON V2.0 PITFALLS TO AVOID

### OpenAI API Issues:
- **Embeddings Model:** Verify `text-embedding-3-small` is still current
- **Rate Limits:** Different limits for chat vs embeddings APIs
- **Batch Processing:** Don't overwhelm API with concurrent requests
- **Error Codes:** Handle 429 (rate limit) and 400 (content policy) properly

### Vector Database Issues:
- **pgvector Extension:** Must be enabled before creating vector columns
- **Dimension Mismatch:** Embedding dimensions must match vector column
- **Index Performance:** Create proper indexes for similarity searches
- **Storage Costs:** Vector storage is expensive, optimize storage

### Bulk Operations Issues:
- **Memory Usage:** Don't load all data into memory at once
- **User Experience:** Show progress for long operations
- **Error Recovery:** Allow retry of failed bulk operations
- **Rate Limiting:** Respect API limits during bulk processing

---

## 🎯 DEVELOPMENT PRIORITIES

1. **Database First:** Set up tables and rollback procedures
2. **API Research:** Verify current documentation for all APIs
3. **Simple Implementation:** Start with basic functionality
4. **Error Handling:** Robust error handling for all operations
5. **User Experience:** Progress indicators and feedback
6. **Performance:** Optimize for bulk operations
7. **Testing:** Test edge cases and failure scenarios

---

## 🛑 TROUBLESHOOTING WORKFLOW (Prevent Getting Stuck)

### **3-Attempt Rule**: 
If working on the same problem for more than 3 attempts based on user feedback:

1. **STOP** making agentic attempts
2. **SWITCH TO PLAN MODE:**
   - **Analyze**: What exactly is wrong?
   - **Logic**: What are we trying to fix and why?
   - **Impact**: How does this affect the overall v2.0 plan?
   - **Strategy**: Clear step-by-step plan to resolve the issue
3. **Present the analysis** to user for agreement
4. **Continue with 3 more attempts** after plan approval

### **Plan Mode Template:**
```
🛑 **SWITCHING TO PLAN MODE** (3+ attempts on same issue)

**Problem Analysis:**
- What's broken: [specific issue]
- Root cause: [why it's happening]
- Current attempts tried: [list what was tried]

**Fix Strategy:**
- Step 1: [specific action]
- Step 2: [specific action]
- Step 3: [specific action]

**Overall Impact:**
- How this affects v2.0 development
- Which phases might be delayed
- Dependencies that are blocked

**Next Steps:**
- Immediate actions needed
- Success criteria to confirm fix
- Backup plan if this approach fails
```

---

## 📝 REFERENCE CHECKLIST

Before any major work:
- [ ] Current API docs checked (web_search)
- [ ] Database rollback plan created
- [ ] `v2-personality-ai-plan.md` phase identified
- [ ] `database-schema.md` updated
- [ ] Deployment state documented
- [ ] Not stuck on same problem for 3+ attempts

**Before ANY commit:**
- [ ] Code Quality: No `any` types, proper interfaces defined
- [ ] ESLint Clean: No unescaped JSX entities, proper const/let usage
- [ ] Build Test: `npm run build` passes locally (recommended)
- [ ] Type Safety: All imports typed correctly

Remember: v2.0 introduces complex AI and vector operations. Take time to research and plan to avoid costly mistakes. 