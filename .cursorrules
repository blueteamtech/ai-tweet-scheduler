# AI Tweet Scheduler - Cursor Rules

## Project Context
This is a beginner-friendly SaaS project building an AI-powered tweet scheduler. The user is learning to code and needs clear explanations and step-by-step guidance.

## Tech Stack
- Frontend: Next.js 14 with TypeScript
- Styling: Tailwind CSS
- Backend: Supabase (database + auth)
- AI: OpenAI API
- Payments: Stripe
- Deployment: Vercel

## Development Guidelines

### For the AI Assistant:
1. **Always explain what you're doing** - Don't just write code, explain why
2. **Use beginner-friendly language** - Avoid jargon without explanation
3. **Show file structure** - Help the user understand where files go
4. **Provide context** - Explain how each piece fits into the bigger picture
5. **Include error handling** - Show proper error handling patterns
6. **Use TypeScript properly** - Define types and interfaces clearly
7. **Follow Next.js 14 App Router patterns** - Use modern Next.js conventions

### Code Standards:
- Use TypeScript for all files
- Use Tailwind CSS for styling
- Follow Next.js App Router structure
- Use proper error boundaries
- Include loading states
- Add proper TypeScript types
- Use environment variables for secrets
- Follow React best practices (hooks, components)

### File Organization:
```
ai-tweet-scheduler/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ app/                 # Next.js App Router pages
‚îÇ   ‚îú‚îÄ‚îÄ components/          # Reusable React components
‚îÇ   ‚îú‚îÄ‚îÄ lib/                # Utility functions, API clients
‚îÇ   ‚îú‚îÄ‚îÄ types/              # TypeScript type definitions
‚îÇ   ‚îî‚îÄ‚îÄ styles/             # Global styles
‚îú‚îÄ‚îÄ public/                 # Static assets
‚îî‚îÄ‚îÄ ...config files
```

### When Writing Code:
1. Always include proper TypeScript types
2. Add comments explaining complex logic
3. Use meaningful variable and function names
4. Include error handling and loading states
5. Make components reusable when possible
6. Follow the principle of separation of concerns

### For Database Design:
- Use Supabase Row Level Security (RLS)
- Create proper foreign key relationships
- Use meaningful table and column names
- Include created_at and updated_at timestamps
- **ALWAYS check database-schema.md before making database assumptions**
- Current tables: `tweets`, `user_twitter_accounts` (see database-schema.md for full details)
- **CRITICAL: Update database-schema.md whenever making database changes (new tables, columns, constraints, etc.)**

### Security Best Practices:
- Never expose API keys in client-side code
- Use environment variables for all secrets
- Implement proper authentication checks
- Use Supabase RLS for data security
- Validate all user inputs

### API Integration Workflow
1.  **Understand the Auth Flow First:** Before writing any code, thoroughly read the API's authentication documentation.
    -   Confirm the exact OAuth version (e.g., OAuth 1.0a, OAuth 2.0).
    -   Identify all required parameters (`state`, `PKCE`, temporary secrets, etc.).
    -   Map out the entire flow on paper or in a comment: `User Clicks -> Our API -> External API -> Callback -> Finalize`.

2.  **Schema Before Code:** Define the database table structure needed to store API credentials (`access_token`, `refresh_token`, `user_id`, etc.) *before* writing the API routes.
    -   Update `database-schema.md` and the `database-setup.sql` file first. This prevents database errors during the callback.

3.  **Build a Debug Route:** For every new external API, create a dedicated server-side debug route (e.g., `/api/debug/new-api-config`).
    -   This route should check that all required environment variables are loaded correctly in the production environment without exposing their values.

4.  **Create a "Dumb" Callback First:** When building the callback handler (e.g., `/api/auth/callback/new-api`), make the first version extremely simple.
    -   It should only do one thing: `console.log()` all the headers and query parameters it receives.
    -   This allows you to instantly see what the external API is sending back, confirming the redirect is working *before* adding complex logic.

5.  **Comprehensive RLS Policies:** When creating a new table that stores user-specific data from an API:
    -   Immediately create RLS policies for `SELECT`, `INSERT`, `UPDATE`, and `DELETE`.
    -   This prevents `406 Not Acceptable` errors and ensures the frontend can both read and write data securely.

### Stripe API Best Practices (2025)
6.  **Webhook Security (Critical):** Always verify webhook signatures to prevent security breaches.
    -   Use the raw request body (UTF-8 encoded) without any modifications for signature verification.
    -   Store webhook secrets securely and rotate them periodically (every 6-12 months).
    -   Implement idempotency checks using event IDs to handle duplicate webhook deliveries.
    -   Return HTTP 200 status codes quickly (within 10 seconds) to prevent retries.

7.  **Payment Flow Architecture:** Design payment flows with proper error handling and user experience.
    -   Use Stripe Checkout for simple implementations, Payment Intents for custom flows.
    -   Always handle asynchronous payment confirmations via webhooks, not just API responses.
    -   Implement proper 3D Secure (SCA) handling for European customers.
    -   Store Stripe customer IDs and payment method IDs, never raw card data.

8.  **Subscription Management:** For SaaS applications, implement robust subscription handling.
    -   Use `checkout.session.completed` and `invoice.payment_succeeded` webhooks for subscription activation.
    -   Handle failed payments gracefully with retry logic and customer communication.
    -   Implement prorated upgrades/downgrades using Stripe's billing features.
    -   Set up usage-based billing with Stripe Metering if applicable.

### OpenAI API Best Practices (2025)
9.  **Rate Limiting & Cost Management:** Implement intelligent request management to avoid surprises.
    -   Use exponential backoff with jitter for rate limit errors (429 status codes).
    -   Set `max_tokens` appropriately to match expected response length (don't over-allocate).
    -   Implement request queuing for batch processing to maximize throughput.
    -   Cache responses for repeated queries to reduce costs and improve performance.

10. **Model Selection & Optimization:** Choose the right model for each task to balance cost and performance.
    -   Use GPT-4o for general tasks, o1 series only for complex reasoning that requires it.
    -   Implement fallback models (e.g., GPT-4o-mini) when primary models hit rate limits.
    -   Use streaming responses for real-time user interfaces to improve perceived performance.
    -   Monitor token usage patterns and adjust limits dynamically based on user tiers.

11. **Security & Content Safety:** Protect against misuse and ensure safe AI interactions.
    -   Never expose API keys in client-side code - always use server-side proxy endpoints.
    -   Implement content filtering for user inputs to prevent policy violations.
    -   Use structured outputs (JSON schema) for reliable data parsing when possible.
    -   Set up monitoring for unusual usage patterns that might indicate abuse.

12. **Error Handling & Resilience:** Build robust applications that gracefully handle AI service issues.
    -   Handle content policy rejections (400 errors) with user-friendly messages.
    -   Implement circuit breakers for repeated API failures to prevent cascading issues.
    -   Use request timeouts (30-60 seconds) to prevent hanging requests.
    -   Log detailed error information for debugging while protecting user privacy.

### When Helping the User:
1. Break down complex tasks into smaller steps
2. Explain the "why" behind each decision
3. Show examples of how to test the code
4. Provide debugging tips when things go wrong
5. Reference the README.md checklist to track progress
6. Suggest best practices for each feature
7. **Environment Files**: Cannot read .env.local files for security reasons - always assume they exist and are configured correctly when user says they are

### üö® CRITICAL GIT WORKFLOW RULES:
8. **Git Commands Directory**: ALWAYS run git commands from the ROOT directory `/AI-Personality-Tweets-to-Scheduler/`, NEVER from `/ai-tweet-scheduler/`
9. **Git Workflow**: Always use `cd AI-Personality-Tweets-to-Scheduler && git add . && git commit -m "message" && git push origin main`
10. **Directory Check**: Before git commands, always run `pwd` to confirm you're in the root directory
11. **NPM vs Git**: NPM commands run from `/ai-tweet-scheduler/`, Git commands run from `/AI-Personality-Tweets-to-Scheduler/`
12. **Deployment Preference**: User prefers Vercel deployment over local testing. ALWAYS suggest pushing to Vercel rather than running npm run dev locally
13. **Directory Reminder**: The user's terminal may be in a different directory than the AI's shell. Always remind them:
    - For Git: "In YOUR terminal, run: cd AI-Personality-Tweets-to-Scheduler && git add . && git commit && git push"
    - Avoid suggesting local npm run dev - user prefers Vercel deployment testing

### üóÑÔ∏è CRITICAL DATABASE WORKFLOW RULES:
14. **Schema Documentation**: ALWAYS update `database-schema.md` when making ANY database changes (tables, columns, constraints, policies)
15. **Schema Verification**: After database changes, provide the verification SQL from `verify-database-schema.sql` for user to run
16. **Schema First**: Check `database-schema.md` BEFORE making assumptions about database structure
17. **Change Log**: Add database changes to "Recent Changes" section in `database-schema.md` with date

### Common Beginner Mistakes to Avoid:
- Not handling loading and error states
- Exposing secrets in client-side code
- Not using TypeScript properly
- Not following Next.js conventions
- Not implementing proper error boundaries
- Not using environment variables
- Not securing API routes

Remember: This is a learning project, so prioritize clarity and education over brevity. 